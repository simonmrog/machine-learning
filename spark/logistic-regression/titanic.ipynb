{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName (\"Titanic Logistic Regression\").getOrCreate ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "891"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the dataset\n",
    "data = spark.read.csv (\"titanic.csv\", inferSchema=True, header=True)\n",
    "\n",
    "data = data.select (['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'])\n",
    "data.count ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "712"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping missing values\n",
    "data = data.na.drop ()\n",
    "data.count ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling categorical values\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorIndexer, VectorAssembler\n",
    "\n",
    "gender_indexer = StringIndexer (inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "embarked_indexer = StringIndexer (inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")\n",
    "\n",
    "encoder = OneHotEncoderEstimator (inputCols=[\"SexIndex\", \"EmbarkedIndex\"], outputCols=[\"SexVec\", \"EmbarkedVec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create assemble for machine learning\n",
    "assembler = VectorAssembler (inputCols=[\"Pclass\", \"SexVec\", \"EmbarkedVec\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- Survived: integer (nullable = true)\n |-- Pclass: integer (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: integer (nullable = true)\n |-- Parch: integer (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- SexIndex: double (nullable = false)\n |-- EmbarkedIndex: double (nullable = false)\n |-- SexVec: vector (nullable = true)\n |-- EmbarkedVec: vector (nullable = true)\n |-- features: vector (nullable = true)\n\nroot\n |-- Survived: integer (nullable = true)\n |-- Pclass: integer (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: integer (nullable = true)\n |-- Parch: integer (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- SexIndex: double (nullable = false)\n |-- EmbarkedIndex: double (nullable = false)\n |-- SexVec: vector (nullable = true)\n |-- EmbarkedVec: vector (nullable = true)\n |-- features: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n"
    }
   ],
   "source": [
    "#without pipelines\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "data1 = gender_indexer.fit (data).transform (data)\n",
    "data1 = embarked_indexer.fit (data1).transform (data1)\n",
    "data1 = encoder.fit (data1).transform (data1)\n",
    "data1 = assembler.transform (data1)\n",
    "data1.printSchema ()\n",
    "\n",
    "#train-test spliting\n",
    "(train1, test1) = data1.randomSplit ([0.7, 0.3])\n",
    "\n",
    "log_reg = LogisticRegression (featuresCol=\"features\", labelCol=\"Survived\")\n",
    "\n",
    "log_reg = log_reg.fit (train1)\n",
    "prediction = log_reg.transform (test1)\n",
    "prediction.printSchema ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipeline and fit the model to data\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "log_reg = LogisticRegression (featuresCol=\"features\", labelCol=\"Survived\")\n",
    "\n",
    "(train, test) = data.randomSplit ([0.7, 0.3])\n",
    "\n",
    "pipeline = Pipeline (stages=[gender_indexer, embarked_indexer, encoder, assembler, log_reg])\n",
    "model = pipeline.fit (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- Survived: integer (nullable = true)\n |-- Pclass: integer (nullable = true)\n |-- Sex: string (nullable = true)\n |-- Age: double (nullable = true)\n |-- SibSp: integer (nullable = true)\n |-- Parch: integer (nullable = true)\n |-- Fare: double (nullable = true)\n |-- Embarked: string (nullable = true)\n |-- SexIndex: double (nullable = false)\n |-- EmbarkedIndex: double (nullable = false)\n |-- SexVec: vector (nullable = true)\n |-- EmbarkedVec: vector (nullable = true)\n |-- features: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n"
    }
   ],
   "source": [
    "#predicting results\n",
    "prediction = model.transform (test)\n",
    "prediction.printSchema ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+--------+----------+\n|Survived|prediction|\n+--------+----------+\n|       0|       1.0|\n|       0|       1.0|\n|       0|       1.0|\n|       0|       1.0|\n|       0|       1.0|\n|       0|       1.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       1.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       0.0|\n|       0|       1.0|\n|       0|       1.0|\n+--------+----------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "prediction.select (\"Survived\", \"prediction\").show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.75\n"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator (rawPredictionCol=\"prediction\", labelCol=\"Survived\")\n",
    "\n",
    "area_under_curve = evaluator.evaluate (prediction)\n",
    "print (area_under_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}